{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4294bed",
   "metadata": {},
   "source": [
    "# Transfer Learning from Custom CNN\n",
    "\n",
    "This notebook implements transfer learning using our best custom CNN as the base model.\n",
    "\n",
    "## Objectives:\n",
    "- Load the best trained custom CNN model\n",
    "- Freeze convolutional layers (keep learned features)\n",
    "- Replace dense layers with new classifier\n",
    "- Fine-tune on the same dataset for improved performance\n",
    "- Evaluate and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a14ec",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54aa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment for Apple Silicon optimization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Configure TensorFlow for Apple Silicon\n",
    "try:\n",
    "    # Enable memory growth for GPU (if available)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU configured: {len(gpus)} device(s) found\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No GPU found, using CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GPU configuration warning: {e}\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Available devices: {[device.name for device in tf.config.list_physical_devices()]}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "print(\"‚úÖ Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44938c7c",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Using the same data setup as the original training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/smithn5/.cache/kagglehub/datasets/alessiocorrado99/animals10/versions/2'\n",
    "data_dir = os.path.join(path, 'raw-img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Keep same as original training\n",
    "FIRST_TIME_SETUP = False  # Data should already be split\n",
    "base_dir = '../data/'\n",
    "batch_size = 32\n",
    "img_height = 224  # Match original training\n",
    "img_width = 224   # Match original training\n",
    "\n",
    "# Paths to best models from previous training\n",
    "BEST_MODEL_PATH = '../models/custom_costum_animals10_acc_0_60.h5'  # Update this to your best model\n",
    "# BEST_MODEL_PATH = '../models/custom_simple_animals10_best.h5'  # Alternative path\n",
    "\n",
    "print(f\"üîç Looking for best model at: {BEST_MODEL_PATH}\")\n",
    "print(f\"üìÇ Data directory: {base_dir}\")\n",
    "print(f\"üñºÔ∏è  Image size: {img_height}x{img_width}\")\n",
    "print(f\"üì¶ Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using tf.keras.preprocessing.image.ImageDataGenerator\n",
    "# Keep same data preprocessing as original training\n",
    "\n",
    "print(\"üîß Creating data generators...\")\n",
    "\n",
    "# Create training data augmentation (same as original)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize to [0,1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create validation/test generator without augmentation\n",
    "test_val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Only rescaling for validation\n",
    ")\n",
    "\n",
    "# Create training dataset with augmentation\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    os.path.join(base_dir, 'train'),\n",
    "    shuffle=True,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create validation dataset WITHOUT augmentation\n",
    "val_ds = test_val_datagen.flow_from_directory(\n",
    "    os.path.join(base_dir, 'val'),\n",
    "    shuffle=False,  # Don't shuffle validation\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create test dataset WITHOUT augmentation\n",
    "test_ds = test_val_datagen.flow_from_directory(\n",
    "    os.path.join(base_dir, 'test'),\n",
    "    shuffle=False,  # Don't shuffle test\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = list(train_ds.class_indices.keys())\n",
    "print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "\n",
    "# Calculate dataset sizes\n",
    "print(f\"Training samples: {train_ds.samples}\")\n",
    "print(f\"Validation samples: {val_ds.samples}\")\n",
    "print(f\"Test samples: {test_ds.samples}\")\n",
    "print(f\"Training batches per epoch: {len(train_ds)}\")\n",
    "print(f\"Validation batches per epoch: {len(val_ds)}\")\n",
    "print(f\"Test batches per epoch: {len(test_ds)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data generators created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d6498",
   "metadata": {},
   "source": [
    "## 3. Load Best Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "print(\"üîÑ Loading best trained model...\")\n",
    "print(f\"Model path: {BEST_MODEL_PATH}\")\n",
    "\n",
    "if not os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"‚ùå Model file not found: {BEST_MODEL_PATH}\")\n",
    "    print(\"\\nüìã Available models in ../models/:\")\n",
    "    models_dir = '../models/'\n",
    "    if os.path.exists(models_dir):\n",
    "        for file in os.listdir(models_dir):\n",
    "            if file.endswith('.h5'):\n",
    "                print(f\"   - {file}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please update BEST_MODEL_PATH to point to your best model\")\n",
    "    raise FileNotFoundError(f\"Model not found: {BEST_MODEL_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Load the complete trained model\n",
    "    base_model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"\\nüìä Loaded Model Information:\")\n",
    "    print(f\"   Input shape: {base_model.input_shape}\")\n",
    "    print(f\"   Output shape: {base_model.output_shape}\")\n",
    "    print(f\"   Total parameters: {base_model.count_params():,}\")\n",
    "    print(f\"   Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in base_model.trainable_weights]):,}\")\n",
    "    \n",
    "    # Evaluate the loaded model to see current performance\n",
    "    print(f\"\\nüß™ Evaluating loaded model on validation set...\")\n",
    "    val_ds.reset()\n",
    "    base_val_loss, base_val_acc = base_model.evaluate(val_ds, verbose=0)\n",
    "    print(f\"   Base model validation accuracy: {base_val_acc:.4f}\")\n",
    "    print(f\"   Base model validation loss: {base_val_loss:.4f}\")\n",
    "    val_ds.reset()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display the architecture\n",
    "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801593d",
   "metadata": {},
   "source": [
    "## 4. Create Transfer Learning Model\n",
    "\n",
    "Extract convolutional layers and create new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the flatten layer to separate conv layers from dense layers\n",
    "print(\"üîç Analyzing model architecture...\")\n",
    "flatten_layer_index = None\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(f\"   Layer {i}: {layer.name} ({type(layer).__name__})\")\n",
    "    if isinstance(layer, tf.keras.layers.Flatten):\n",
    "        flatten_layer_index = i\n",
    "        print(f\"   ‚úÖ Found Flatten layer at index {i}\")\n",
    "        break\n",
    "\n",
    "if flatten_layer_index is None:\n",
    "    print(\"‚ùå No Flatten layer found! Model structure might be different.\")\n",
    "    print(\"   Looking for GlobalAveragePooling2D or other pooling layers...\")\n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        if isinstance(layer, (tf.keras.layers.GlobalAveragePooling2D, tf.keras.layers.GlobalMaxPooling2D)):\n",
    "            flatten_layer_index = i\n",
    "            print(f\"   ‚úÖ Found {type(layer).__name__} layer at index {i}\")\n",
    "            break\n",
    "\n",
    "if flatten_layer_index is None:\n",
    "    raise ValueError(\"Could not find a suitable layer to separate conv and dense parts\")\n",
    "\n",
    "print(f\"\\nüéØ Will keep layers 0 to {flatten_layer_index} (convolutional part)\")\n",
    "print(f\"   Will replace layers {flatten_layer_index+1} onwards (dense part)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extractor (convolutional part)\n",
    "print(\"üîß Creating feature extractor from convolutional layers...\")\n",
    "\n",
    "# Extract the convolutional base (up to and including flatten/pooling)\n",
    "conv_base = tf.keras.Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model.layers[flatten_layer_index].output\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Feature extractor created:\")\n",
    "print(f\"   Input shape: {conv_base.input_shape}\")\n",
    "print(f\"   Output shape: {conv_base.output_shape}\")\n",
    "print(f\"   Parameters: {conv_base.count_params():,}\")\n",
    "\n",
    "# Freeze the convolutional base\n",
    "conv_base.trainable = False\n",
    "print(f\"\\n‚ùÑÔ∏è  Convolutional base frozen (trainable=False)\")\n",
    "print(f\"   Frozen parameters: {conv_base.count_params():,}\")\n",
    "\n",
    "# Display the feature extractor architecture\n",
    "print(f\"\\nüèóÔ∏è  Feature Extractor Architecture:\")\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89407e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new transfer learning model\n",
    "print(\"üöÄ Building transfer learning model with new classifier...\")\n",
    "\n",
    "num_classes = len(class_names)\n",
    "input_shape = (img_height, img_width, 3)\n",
    "\n",
    "# Build the new model\n",
    "transfer_model = tf.keras.Sequential([\n",
    "    conv_base,  # Frozen convolutional base\n",
    "    \n",
    "    # New classifier head - you can experiment with different architectures\n",
    "    tf.keras.layers.GlobalAveragePooling2D() if not isinstance(base_model.layers[flatten_layer_index], tf.keras.layers.GlobalAveragePooling2D) else tf.keras.layers.Lambda(lambda x: x),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax', name='predictions')\n",
    "])\n",
    "\n",
    "# Build the model by calling it with a sample input\n",
    "transfer_model.build(input_shape=(None, img_height, img_width, 3))\n",
    "\n",
    "print(f\"‚úÖ Transfer learning model created!\")\n",
    "print(f\"\\nüìä Transfer Learning Model Information:\")\n",
    "print(f\"   Input shape: {transfer_model.input_shape}\")\n",
    "print(f\"   Output shape: {transfer_model.output_shape}\")\n",
    "print(f\"   Total parameters: {transfer_model.count_params():,}\")\n",
    "print(f\"   Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights]):,}\")\n",
    "print(f\"   Frozen parameters: {transfer_model.count_params() - sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights]):,}\")\n",
    "\n",
    "# Display the full architecture\n",
    "print(f\"\\nüèóÔ∏è  Complete Transfer Learning Model:\")\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57a2a7",
   "metadata": {},
   "source": [
    "## 5. Model Compilation and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the transfer learning model\n",
    "print(\"‚öôÔ∏è  Compiling transfer learning model...\")\n",
    "\n",
    "# Use a lower learning rate for transfer learning\n",
    "LEARNING_RATE = 0.0001  # Lower LR since we're fine-tuning\n",
    "\n",
    "transfer_model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model compiled with learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Setup callbacks for training\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/transfer_learning_best.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Training callbacks configured:\")\n",
    "print(\"   - EarlyStopping: patience=15\")\n",
    "print(\"   - ModelCheckpoint: saves best model\")\n",
    "print(\"   - ReduceLROnPlateau: factor=0.5, patience=5\")\n",
    "\n",
    "# Test the model with a sample batch to ensure everything works\n",
    "print(\"\\nüß™ Testing model with sample batch...\")\n",
    "sample_batch_x, sample_batch_y = next(train_ds)\n",
    "test_prediction = transfer_model.predict(sample_batch_x[:1], verbose=0)\n",
    "print(f\"   Sample prediction shape: {test_prediction.shape}\")\n",
    "print(f\"   Sample prediction sum: {test_prediction.sum():.4f} (should be ~1.0)\")\n",
    "train_ds.reset()\n",
    "print(\"‚úÖ Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4e8ef",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbca9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 30  # Usually fewer epochs needed for transfer learning\n",
    "\n",
    "print(f\"üöÄ Starting transfer learning training...\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"- Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"- Max epochs: {EPOCHS}\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Frozen layers: {len(conv_base.layers)} convolutional layers\")\n",
    "print(f\"- Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights]):,}\")\n",
    "print(f\"- Total parameters: {transfer_model.count_params():,}\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Start training\n",
    "history = transfer_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Transfer learning training completed!\")\n",
    "if len(history.history['accuracy']) > 0:\n",
    "    print(f\"Total epochs trained: {len(history.history['accuracy'])}\")\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    # Compare with base model\n",
    "    improvement = max(history.history['val_accuracy']) - base_val_acc\n",
    "    print(f\"\\nüìà Performance Improvement:\")\n",
    "    print(f\"   Base model validation accuracy: {base_val_acc:.4f}\")\n",
    "    print(f\"   Transfer model best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"   Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "\n",
    "# Reset generators for future use\n",
    "train_ds.reset()\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a893ca",
   "metadata": {},
   "source": [
    "## 7. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transfer_learning_history(history, base_val_acc):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics for transfer learning with comparison to base model.\n",
    "    \"\"\"\n",
    "    # Create a larger figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    ax1.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "    ax1.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    # Add base model performance line\n",
    "    ax1.axhline(y=base_val_acc, color='orange', linestyle='--', linewidth=2, label=f'Base Model Val Acc ({base_val_acc:.3f})')\n",
    "    \n",
    "    ax1.set_title('Transfer Learning - Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add best accuracy annotation\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_val_acc_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "    ax1.annotate(f'Best: {best_val_acc:.3f}', \n",
    "                xy=(best_val_acc_epoch, best_val_acc), \n",
    "                xytext=(best_val_acc_epoch + 2, best_val_acc - 0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(epochs, history.history['loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    ax2.set_title('Transfer Learning - Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        ax3.plot(epochs, history.history['lr'], 'g-', label='Learning Rate', linewidth=2)\n",
    "        ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Plot improvement over base model\n",
    "        improvement = np.array(history.history['val_accuracy']) - base_val_acc\n",
    "        ax3.plot(epochs, improvement, 'green', label='Improvement over Base Model', linewidth=2)\n",
    "        ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='Base Model Level')\n",
    "        ax3.set_title('Transfer Learning Improvement', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Accuracy Improvement')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot comparison summary\n",
    "    ax4.text(0.1, 0.9, 'Transfer Learning Summary', fontsize=16, fontweight='bold', transform=ax4.transAxes)\n",
    "    ax4.text(0.1, 0.8, f'Base Model Val Acc: {base_val_acc:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "    ax4.text(0.1, 0.7, f'Best Transfer Val Acc: {best_val_acc:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "    ax4.text(0.1, 0.6, f'Improvement: {best_val_acc - base_val_acc:+.4f}', fontsize=12, transform=ax4.transAxes, \n",
    "             color='green' if best_val_acc > base_val_acc else 'red')\n",
    "    ax4.text(0.1, 0.5, f'Epochs Trained: {len(epochs)}', fontsize=12, transform=ax4.transAxes)\n",
    "    ax4.text(0.1, 0.4, f'Final Train Acc: {history.history[\"accuracy\"][-1]:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "    ax4.text(0.1, 0.3, f'Final Val Acc: {history.history[\"val_accuracy\"][-1]:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "    \n",
    "    # Add performance assessment\n",
    "    if best_val_acc > base_val_acc + 0.01:\n",
    "        ax4.text(0.1, 0.2, '‚úÖ Transfer Learning Successful!', fontsize=12, transform=ax4.transAxes, color='green')\n",
    "    elif best_val_acc > base_val_acc:\n",
    "        ax4.text(0.1, 0.2, '‚ö° Marginal Improvement', fontsize=12, transform=ax4.transAxes, color='orange')\n",
    "    else:\n",
    "        ax4.text(0.1, 0.2, '‚ö†Ô∏è  No Improvement', fontsize=12, transform=ax4.transAxes, color='red')\n",
    "    \n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRANSFER LEARNING RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Base model validation accuracy: {base_val_acc:.4f}\")\n",
    "    print(f\"Transfer learning best accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Improvement: {best_val_acc - base_val_acc:+.4f} ({(best_val_acc - base_val_acc)*100:+.2f}%)\")\n",
    "    print(f\"Total epochs trained: {len(epochs)}\")\n",
    "    print(f\"Best epoch: {best_val_acc_epoch}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    final_gap = history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Final accuracy gap: {final_gap:.4f}\")\n",
    "    if final_gap > 0.1:\n",
    "        print(\"‚ö†Ô∏è  Potential overfitting detected\")\n",
    "    elif final_gap > 0.05:\n",
    "        print(\"‚ö° Mild overfitting\")\n",
    "    else:\n",
    "        print(\"‚úÖ Good generalization\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Plot the training history\n",
    "plot_transfer_learning_history(history, base_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07252cf",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_ds.reset()\n",
    "val_loss, val_accuracy = transfer_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Transfer Learning Model - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Transfer Learning Model - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Generate predictions on validation set\n",
    "print(\"\\nGenerating predictions...\")\n",
    "val_ds.reset()\n",
    "y_pred = transfer_model.predict(val_ds, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels from validation generator\n",
    "val_ds.reset()\n",
    "y_true_classes = []\n",
    "for i in range(len(val_ds)):\n",
    "    batch_images, batch_labels = next(val_ds)\n",
    "    batch_true_classes = np.argmax(batch_labels, axis=1)\n",
    "    y_true_classes.extend(batch_true_classes)\n",
    "\n",
    "y_true_classes = np.array(y_true_classes)\n",
    "\n",
    "# Ensure we have the same number of predictions and true labels\n",
    "min_length = min(len(y_pred_classes), len(y_true_classes))\n",
    "y_pred_classes = y_pred_classes[:min_length]\n",
    "y_true_classes = y_true_classes[:min_length]\n",
    "\n",
    "print(f\"Number of samples evaluated: {min_length}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report - Transfer Learning Model:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Reset generator\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9dd27",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Transfer Learning Model', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy - Transfer Learning Model:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {class_accuracy[i]:.4f}\")\n",
    "\n",
    "# Calculate and display macro and weighted averages\n",
    "macro_avg = np.mean(class_accuracy)\n",
    "weighted_avg = np.average(class_accuracy, weights=cm.sum(axis=1))\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"Macro average accuracy: {macro_avg:.4f}\")\n",
    "print(f\"Weighted average accuracy: {weighted_avg:.4f}\")\n",
    "print(f\"Overall accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc142af",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs transfer learning model\n",
    "print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base Model (Original CNN):\")\n",
    "print(f\"   Validation Accuracy: {base_val_acc:.4f}\")\n",
    "print(f\"   Total Parameters: {base_model.count_params():,}\")\n",
    "print(f\"   All parameters were trainable\")\n",
    "\n",
    "print(f\"\\nTransfer Learning Model:\")\n",
    "print(f\"   Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"   Total Parameters: {transfer_model.count_params():,}\")\n",
    "print(f\"   Trainable Parameters: {sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights]):,}\")\n",
    "print(f\"   Frozen Parameters: {transfer_model.count_params() - sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights]):,}\")\n",
    "\n",
    "improvement = val_accuracy - base_val_acc\n",
    "print(f\"\\nPerformance Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "\n",
    "if improvement > 0.01:\n",
    "    print(\"‚úÖ Transfer learning was successful!\")\n",
    "elif improvement > 0:\n",
    "    print(\"‚ö° Marginal improvement achieved\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No significant improvement - consider different approach\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the transfer learning model\n",
    "val_accuracy_str = f\"{val_accuracy:.2f}\".replace('.', '_')\n",
    "model_filename = f'../models/transfer_learning_animals10_acc_{val_accuracy_str}.h5'\n",
    "transfer_model.save(model_filename)\n",
    "print(f\"\\nüíæ Transfer learning model saved as: {model_filename}\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open(f'../models/transfer_learning_animals10_acc_{val_accuracy_str}_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"üìà Training history saved\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_name': 'Transfer Learning from Custom CNN',\n",
    "    'base_model_path': BEST_MODEL_PATH,\n",
    "    'base_model_accuracy': float(base_val_acc),\n",
    "    'dataset': 'Animals10',\n",
    "    'input_shape': list(input_shape),\n",
    "    'num_classes': num_classes,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs_trained': len(history.history['accuracy']),\n",
    "    'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "    'final_val_accuracy': float(val_accuracy),\n",
    "    'improvement_over_base': float(improvement),\n",
    "    'total_parameters': int(transfer_model.count_params()),\n",
    "    'trainable_parameters': int(sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights])),\n",
    "    'frozen_parameters': int(transfer_model.count_params() - sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights])),\n",
    "    'architecture': 'Custom CNN base + new classifier head'\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'../models/transfer_learning_animals10_acc_{val_accuracy_str}_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"‚öôÔ∏è  Model configuration saved\")\n",
    "print(f\"\\nüéâ Transfer learning experiment completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a10ee2",
   "metadata": {},
   "source": [
    "## 11. Optional: Fine-tuning the Entire Model\n",
    "\n",
    "Uncomment and run this section if you want to unfreeze some layers and fine-tune the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80019d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Fine-tune the entire model\n",
    "# print(\"üîì Unfreezing convolutional base for fine-tuning...\")\n",
    "\n",
    "# # Unfreeze the convolutional base\n",
    "# conv_base.trainable = True\n",
    "\n",
    "# # Use a much lower learning rate for fine-tuning\n",
    "# FINE_TUNE_LR = 0.00001\n",
    "\n",
    "# transfer_model.compile(\n",
    "#     optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=FINE_TUNE_LR),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# print(f\"‚úÖ Model recompiled for fine-tuning with LR: {FINE_TUNE_LR}\")\n",
    "# print(f\"   Now trainable parameters: {sum([tf.keras.backend.count_params(w) for w in transfer_model.trainable_weights]):,}\")\n",
    "\n",
    "# # Fine-tune for a few more epochs\n",
    "# FINE_TUNE_EPOCHS = 10\n",
    "\n",
    "# print(f\"\\nüéØ Starting fine-tuning for {FINE_TUNE_EPOCHS} epochs...\")\n",
    "# fine_tune_history = transfer_model.fit(\n",
    "#     train_ds,\n",
    "#     epochs=FINE_TUNE_EPOCHS,\n",
    "#     validation_data=val_ds,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "#         ModelCheckpoint('../models/fine_tuned_best.h5', save_best_only=True)\n",
    "#     ],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Fine-tuning completed!\")\n",
    "# final_val_loss, final_val_acc = transfer_model.evaluate(val_ds, verbose=0)\n",
    "# print(f\"Final fine-tuned accuracy: {final_val_acc:.4f}\")\n",
    "# print(f\"Total improvement from base: {final_val_acc - base_val_acc:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
