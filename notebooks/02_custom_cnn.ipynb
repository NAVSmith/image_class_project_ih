{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9023292",
   "metadata": {},
   "source": [
    "# Custom CNN Architecture for Image Classification\n",
    "\n",
    "This notebook implements and trains a custom CNN architecture from scratch.\n",
    "\n",
    "## Objectives:\n",
    "- Design custom CNN architecture\n",
    "- Implement data preprocessing and augmentation\n",
    "- Train the model with proper validation\n",
    "- Evaluate performance and visualize results\n",
    "- Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a473d",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cea7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU configured: 1 device(s) found\n",
      "TensorFlow version: 2.15.0\n",
      "Available devices: ['/physical_device:CPU:0', '/physical_device:GPU:0']\n",
      "‚úÖ Random seeds set for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Configure environment for Apple Silicon optimization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Configure TensorFlow for Apple Silicon\n",
    "try:\n",
    "    # Enable memory growth for GPU (if available)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU configured: {len(gpus)} device(s) found\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No GPU found, using CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GPU configuration warning: {e}\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Available devices: {[device.name for device in tf.config.list_physical_devices()]}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(\"‚úÖ Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db73c0c",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bb04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/smithn5/.cache/kagglehub/datasets/alessiocorrado99/animals10/versions/2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d220e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating improved data generators...\n",
      "Found 23565 images belonging to 10 classes.\n",
      "Found 23565 images belonging to 10 classes.\n",
      "Found 2614 images belonging to 10 classes.\n",
      "Found 2614 images belonging to 10 classes.\n",
      "Found 10 classes: ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
      "Training samples: 23565\n",
      "Validation samples: 2614\n",
      "Training batches per epoch: 737\n",
      "Validation batches per epoch: 82\n",
      "Batch size: 32\n",
      "\n",
      "Class distribution check:\n",
      "  cane: 4863 images\n",
      "  cavallo: 2623 images\n",
      "  elefante: 1446 images\n",
      "  farfalla: 2112 images\n",
      "  gallina: 3098 images\n",
      "  gatto: 1668 images\n",
      "  mucca: 1866 images\n",
      "  pecora: 1820 images\n",
      "  ragno: 4821 images\n",
      "  scoiattolo: 1862 images\n",
      "\n",
      "Class balance analysis:\n",
      "  Min class size: 1446\n",
      "  Max class size: 4863\n",
      "  Imbalance ratio: 3.36\n",
      "‚ö° Moderate class imbalance detected\n",
      "\n",
      "‚úÖ Improved data generators created with:\n",
      "  - Reduced augmentation intensity\n",
      "  - Separate validation generator (no augmentation)\n",
      "  - Proper class balance verification\n",
      "Found 10 classes: ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
      "Training samples: 23565\n",
      "Validation samples: 2614\n",
      "Training batches per epoch: 737\n",
      "Validation batches per epoch: 82\n",
      "Batch size: 32\n",
      "\n",
      "Class distribution check:\n",
      "  cane: 4863 images\n",
      "  cavallo: 2623 images\n",
      "  elefante: 1446 images\n",
      "  farfalla: 2112 images\n",
      "  gallina: 3098 images\n",
      "  gatto: 1668 images\n",
      "  mucca: 1866 images\n",
      "  pecora: 1820 images\n",
      "  ragno: 4821 images\n",
      "  scoiattolo: 1862 images\n",
      "\n",
      "Class balance analysis:\n",
      "  Min class size: 1446\n",
      "  Max class size: 4863\n",
      "  Imbalance ratio: 3.36\n",
      "‚ö° Moderate class imbalance detected\n",
      "\n",
      "‚úÖ Improved data generators created with:\n",
      "  - Reduced augmentation intensity\n",
      "  - Separate validation generator (no augmentation)\n",
      "  - Proper class balance verification\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using tf.keras.preprocessing.image.ImageDataGenerator\n",
    "batch_size = 32\n",
    "img_height = 224 # to work with vgg16 models\n",
    "img_width = 224 # to work with vgg16 models\n",
    "\n",
    "# ISSUE DIAGNOSIS: Your augmentation might be too aggressive!\n",
    "# Let's create a less aggressive augmentation setup\n",
    "\n",
    "print(\"üîß Creating improved data generators...\")\n",
    "\n",
    "# Create LESS aggressive augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,  # Normalize to [0,1]\n",
    "    rotation_range=10,      # Reduced from 20\n",
    "    width_shift_range=0.1,  # Reduced from 0.2\n",
    "    height_shift_range=0.1, # Reduced from 0.2\n",
    "    shear_range=0.1,        # Reduced from 0.2\n",
    "    zoom_range=0.1,         # Reduced from 0.2\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Create validation generator without augmentation (IMPORTANT!)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,  # Only rescaling for validation\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Create training dataset with augmentation\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    os.path.join(path, 'raw-img'),\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create validation dataset WITHOUT augmentation\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "    os.path.join(path, 'raw-img'),\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,  # Don't shuffle validation\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = list(train_ds.class_indices.keys())\n",
    "print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "\n",
    "# Calculate dataset sizes\n",
    "print(f\"Training samples: {train_ds.samples}\")\n",
    "print(f\"Validation samples: {val_ds.samples}\")\n",
    "print(f\"Training batches per epoch: {len(train_ds)}\")\n",
    "print(f\"Validation batches per epoch: {len(val_ds)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Verify class balance\n",
    "print(f\"\\nClass distribution check:\")\n",
    "class_counts = {}\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(path, 'raw-img', class_name)\n",
    "    if os.path.exists(class_dir):\n",
    "        count = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        class_counts[class_name] = count\n",
    "        print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "# Check for extremely imbalanced classes\n",
    "min_count = min(class_counts.values())\n",
    "max_count = max(class_counts.values())\n",
    "imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "print(f\"\\nClass balance analysis:\")\n",
    "print(f\"  Min class size: {min_count}\")\n",
    "print(f\"  Max class size: {max_count}\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Severe class imbalance detected!\")\n",
    "    print(\"   This could explain poor learning performance\")\n",
    "elif imbalance_ratio > 3:\n",
    "    print(\"‚ö° Moderate class imbalance detected\")\n",
    "else:\n",
    "    print(\"‚úÖ Classes are reasonably balanced\")\n",
    "\n",
    "print(f\"\\n‚úÖ Improved data generators created with:\")\n",
    "print(f\"  - Reduced augmentation intensity\")\n",
    "print(f\"  - Separate validation generator (no augmentation)\")\n",
    "print(f\"  - Proper class balance verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e2bd",
   "metadata": {},
   "source": [
    "## 3. Custom CNN Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e75002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14982474 (57.15 MB)\n",
      "Trainable params: 14982474 (57.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Model Details:\n",
      "Input shape: (224, 224, 3)\n",
      "Number of classes: 10\n",
      "Total parameters: 14,982,474\n",
      "Trainable parameters: 14,982,474\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "‚úÖ Model architecture diagram saved to '../models/custom_vgg16_architecture.png'\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14982474 (57.15 MB)\n",
      "Trainable params: 14982474 (57.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Model Details:\n",
      "Input shape: (224, 224, 3)\n",
      "Number of classes: 10\n",
      "Total parameters: 14,982,474\n",
      "Trainable parameters: 14,982,474\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "‚úÖ Model architecture diagram saved to '../models/custom_vgg16_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "# Build custom CNN model based on VGG16 architecture\n",
    "def create_custom_vgg16_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a custom CNN following VGG16 architecture.\n",
    "    \n",
    "    Architecture: 5 convolutional blocks + Global Average Pooling + Dense layers\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1 - 64 filters\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'),\n",
    "        \n",
    "        # Block 2 - 128 filters\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'),\n",
    "        \n",
    "        # Block 3 - 256 filters\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1'),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2'),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'),\n",
    "        \n",
    "        # Block 4 - 512 filters\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3'),\n",
    "        layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'),\n",
    "        \n",
    "        # Block 5 - 512 filters\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'),\n",
    "        \n",
    "        # Classification head (replacing VGG16's fully connected layers)\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (img_height, img_width, 3)  # (224, 224, 3) for VGG16-like input\n",
    "num_classes = len(class_names)\n",
    "custom_model = create_custom_vgg16_cnn(input_shape, num_classes)\n",
    "\n",
    "# Display model architecture\n",
    "custom_model.summary()\n",
    "\n",
    "print(f\"\\nModel Details:\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Total parameters: {custom_model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in custom_model.trainable_weights]):,}\")\n",
    "\n",
    "# Visualize model architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    custom_model, \n",
    "    to_file='../models/custom_vgg16_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n",
    "print(\"‚úÖ Model architecture diagram saved to '../models/custom_vgg16_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f069f0",
   "metadata": {},
   "source": [
    "## 4. Model Compilation and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e427d2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with:\n",
      "- Optimizer: Adam (lr=0.001)\n",
      "- Loss: Categorical Crossentropy\n",
      "- Metrics: Accuracy\n",
      "- Callbacks: Early Stopping, Model Checkpoint, LR Reduction\n",
      "- Model checkpoint will be saved to: '../models/custom_vgg16_animals10_best.h5'\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "custom_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/custom_vgg16_animals10_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Model compiled with:\")\n",
    "print(f\"- Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"- Loss: Categorical Crossentropy\")\n",
    "print(f\"- Metrics: Accuracy\")\n",
    "print(f\"- Callbacks: Early Stopping, Model Checkpoint, LR Reduction\")\n",
    "print(f\"- Model checkpoint will be saved to: '../models/custom_vgg16_animals10_best.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d267e",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEBUGGING MODEL AND DATA...\n",
      "==================================================\n",
      "Data Generator Check:\n",
      "Training samples: 23565\n",
      "Validation samples: 2614\n",
      "Classes found: 10\n",
      "Class names: ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
      "\n",
      "Sample batch shapes:\n",
      "Images: (32, 224, 224, 3)\n",
      "Labels: (32, 10)\n",
      "Image value range: [0.000, 1.000]\n",
      "Labels sum (should be 1 for each): [1. 1. 1. 1. 1.]\n",
      "\n",
      "Model check:\n",
      "Model expects input shape: (None, 224, 224, 3)\n",
      "Model outputs shape: (None, 10)\n",
      "Number of parameters: 14,982,474\n",
      "\n",
      "Sample batch shapes:\n",
      "Images: (32, 224, 224, 3)\n",
      "Labels: (32, 10)\n",
      "Image value range: [0.000, 1.000]\n",
      "Labels sum (should be 1 for each): [1. 1. 1. 1. 1.]\n",
      "\n",
      "Model check:\n",
      "Model expects input shape: (None, 224, 224, 3)\n",
      "Model outputs shape: (None, 10)\n",
      "Number of parameters: 14,982,474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction shape: (1, 10)\n",
      "Test prediction sum: 1.0000001192092896 (should be ~1.0)\n",
      "Test prediction values: [0.1000734  0.10008384 0.09992044 0.09992121 0.10001957]...\n",
      "\n",
      "==================================================\n",
      "üöÄ STARTING IMPROVED TRAINING...\n",
      "==================================================\n",
      "‚úÖ Reduced learning rate to 0.0001\n",
      "Improved training setup:\n",
      "- Learning rate: 0.0001 (reduced from 0.001)\n",
      "- Max epochs: 20\n",
      "- Early stopping patience: 8\n",
      "- LR reduction factor: 0.2 (more aggressive)\n",
      "- Batch size: 32\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:30:06.364158: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/737 [==============================] - ETA: 0s - loss: 2.1816 - accuracy: 0.1970\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24637, saving model to ../models/custom_vgg16_animals10_best.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24637, saving model to ../models/custom_vgg16_animals10_best.h5\n",
      "737/737 [==============================] - 883s 1s/step - loss: 2.1816 - accuracy: 0.1970 - val_loss: 2.0829 - val_accuracy: 0.2464 - lr: 1.0000e-04\n",
      "737/737 [==============================] - 883s 1s/step - loss: 2.1816 - accuracy: 0.1970 - val_loss: 2.0829 - val_accuracy: 0.2464 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "Epoch 2/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 1.9767 - accuracy: 0.2824\n",
      "Epoch 2: val_accuracy improved from 0.24637 to 0.34545, saving model to ../models/custom_vgg16_animals10_best.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.24637 to 0.34545, saving model to ../models/custom_vgg16_animals10_best.h5\n",
      "737/737 [==============================] - 874s 1s/step - loss: 1.9767 - accuracy: 0.2824 - val_loss: 1.7922 - val_accuracy: 0.3454 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "737/737 [==============================] - 874s 1s/step - loss: 1.9767 - accuracy: 0.2824 - val_loss: 1.7922 - val_accuracy: 0.3454 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 1.6990 - accuracy: 0.3914  \n",
      "Epoch 3: val_accuracy improved from 0.34545 to 0.44912, saving model to ../models/custom_vgg16_animals10_best.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.34545 to 0.44912, saving model to ../models/custom_vgg16_animals10_best.h5\n",
      "737/737 [==============================] - 50538s 69s/step - loss: 1.6990 - accuracy: 0.3914 - val_loss: 1.5766 - val_accuracy: 0.4491 - lr: 1.0000e-04\n",
      "737/737 [==============================] - 50538s 69s/step - loss: 1.6990 - accuracy: 0.3914 - val_loss: 1.5766 - val_accuracy: 0.4491 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "Epoch 4/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 2.2667 - accuracy: 0.3496 \n",
      "Epoch 4: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 9729s 13s/step - loss: 2.2667 - accuracy: 0.3496 - val_loss: 1.6808 - val_accuracy: 0.4044 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 9729s 13s/step - loss: 2.2667 - accuracy: 0.3496 - val_loss: 1.6808 - val_accuracy: 0.4044 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "Epoch 5/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 2.5673 - accuracy: 0.2098\n",
      "Epoch 5: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 886s 1s/step - loss: 2.5673 - accuracy: 0.2098 - val_loss: 2.1391 - val_accuracy: 0.2131 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 886s 1s/step - loss: 2.5673 - accuracy: 0.2098 - val_loss: 2.1391 - val_accuracy: 0.2131 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "Epoch 6/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 2.0485 - accuracy: 0.2573\n",
      "Epoch 6: val_accuracy did not improve from 0.44912\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "737/737 [==============================] - 885s 1s/step - loss: 2.0485 - accuracy: 0.2573 - val_loss: 1.9612 - val_accuracy: 0.2884 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.44912\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "737/737 [==============================] - 885s 1s/step - loss: 2.0485 - accuracy: 0.2573 - val_loss: 1.9612 - val_accuracy: 0.2884 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "Epoch 7/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 1.8671 - accuracy: 0.3368\n",
      "Epoch 7: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 1137s 2s/step - loss: 1.8671 - accuracy: 0.3368 - val_loss: 2.1890 - val_accuracy: 0.2789 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 1137s 2s/step - loss: 1.8671 - accuracy: 0.3368 - val_loss: 2.1890 - val_accuracy: 0.2789 - lr: 2.0000e-05\n",
      "Epoch 8/20\n",
      "Epoch 8/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 2.0167 - accuracy: 0.2976\n",
      "Epoch 8: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 892s 1s/step - loss: 2.0167 - accuracy: 0.2976 - val_loss: 2.0660 - val_accuracy: 0.2923 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 892s 1s/step - loss: 2.0167 - accuracy: 0.2976 - val_loss: 2.0660 - val_accuracy: 0.2923 - lr: 2.0000e-05\n",
      "Epoch 9/20\n",
      "Epoch 9/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 1.9767 - accuracy: 0.3095\n",
      "Epoch 9: val_accuracy did not improve from 0.44912\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "737/737 [==============================] - 894s 1s/step - loss: 1.9767 - accuracy: 0.3095 - val_loss: 2.0156 - val_accuracy: 0.3053 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.44912\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "737/737 [==============================] - 894s 1s/step - loss: 1.9767 - accuracy: 0.3095 - val_loss: 2.0156 - val_accuracy: 0.3053 - lr: 2.0000e-05\n",
      "Epoch 10/20\n",
      "Epoch 10/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 1.8685 - accuracy: 0.3391\n",
      "Epoch 10: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 899s 1s/step - loss: 1.8685 - accuracy: 0.3391 - val_loss: 1.9257 - val_accuracy: 0.3298 - lr: 4.0000e-06\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 899s 1s/step - loss: 1.8685 - accuracy: 0.3391 - val_loss: 1.9257 - val_accuracy: 0.3298 - lr: 4.0000e-06\n",
      "Epoch 11/20\n",
      "Epoch 11/20\n",
      "737/737 [==============================] - ETA: 0s - loss: 1.8459 - accuracy: 0.3436Restoring model weights from the end of the best epoch: 3.\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 884s 1s/step - loss: 1.8459 - accuracy: 0.3436 - val_loss: 1.8992 - val_accuracy: 0.3355 - lr: 4.0000e-06\n",
      "Epoch 11: early stopping\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.44912\n",
      "737/737 [==============================] - 884s 1s/step - loss: 1.8459 - accuracy: 0.3436 - val_loss: 1.8992 - val_accuracy: 0.3355 - lr: 4.0000e-06\n",
      "Epoch 11: early stopping\n",
      "\n",
      "‚úÖ Training completed!\n",
      "Total epochs trained: 11\n",
      "Best validation accuracy: 0.4491\n",
      "Final training accuracy: 0.3436\n",
      "Final validation accuracy: 0.3355\n",
      "\n",
      "‚úÖ Training completed!\n",
      "Total epochs trained: 11\n",
      "Best validation accuracy: 0.4491\n",
      "Final training accuracy: 0.3436\n",
      "Final validation accuracy: 0.3355\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Let's first debug the data and model setup\n",
    "print(\"üîç DEBUGGING MODEL AND DATA...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check data generators\n",
    "print(\"Data Generator Check:\")\n",
    "print(f\"Training samples: {train_ds.samples}\")\n",
    "print(f\"Validation samples: {val_ds.samples}\")\n",
    "print(f\"Classes found: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Test a batch to see if data looks correct\n",
    "sample_batch_x, sample_batch_y = next(train_ds)\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"Images: {sample_batch_x.shape}\")\n",
    "print(f\"Labels: {sample_batch_y.shape}\")\n",
    "print(f\"Image value range: [{sample_batch_x.min():.3f}, {sample_batch_x.max():.3f}]\")\n",
    "print(f\"Labels sum (should be 1 for each): {sample_batch_y.sum(axis=1)[:5]}\")\n",
    "train_ds.reset()\n",
    "\n",
    "# Check model output\n",
    "print(f\"\\nModel check:\")\n",
    "print(f\"Model expects input shape: {custom_model.input_shape}\")\n",
    "print(f\"Model outputs shape: {custom_model.output_shape}\")\n",
    "print(f\"Number of parameters: {custom_model.count_params():,}\")\n",
    "\n",
    "# Test model prediction on one batch\n",
    "test_pred = custom_model.predict(sample_batch_x[:1], verbose=0)\n",
    "print(f\"Test prediction shape: {test_pred.shape}\")\n",
    "print(f\"Test prediction sum: {test_pred.sum()} (should be ~1.0)\")\n",
    "print(f\"Test prediction values: {test_pred[0][:5]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING IMPROVED TRAINING...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# SOLUTION 1: Lower learning rate significantly\n",
    "custom_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Much lower LR\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(\"‚úÖ Reduced learning rate to 0.0001\")\n",
    "\n",
    "# SOLUTION 2: Improved callbacks with more aggressive learning rate reduction\n",
    "improved_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=8,  # Reduced patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/custom_vgg16_animals10_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,  # More aggressive reduction\n",
    "        patience=3,  # Faster response\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 20  # Increased epochs since we lowered LR\n",
    "\n",
    "print(f\"Improved training setup:\")\n",
    "print(f\"- Learning rate: 0.0001 (reduced from 0.001)\")\n",
    "print(f\"- Max epochs: {EPOCHS}\")\n",
    "print(f\"- Early stopping patience: 8\")\n",
    "print(f\"- LR reduction factor: 0.2 (more aggressive)\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "\n",
    "# Train with improved settings\n",
    "history = custom_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=len(train_ds),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=len(val_ds),\n",
    "    callbacks=improved_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "if len(history.history['accuracy']) > 0:\n",
    "    print(f\"Total epochs trained: {len(history.history['accuracy'])}\")\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Reset generators for future use\n",
    "train_ds.reset()\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING WITH SIMPLE MODEL FIRST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple model parameters: 24,202\n",
      "Training simple model for 3 epochs to test data pipeline...\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# Alternative approach: Create a simpler model first to test if data is the issue\n",
    "def create_simple_test_model(input_shape, num_classes):\n",
    "    \"\"\"Create a very simple model to test if the data pipeline works\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test with simple model first\n",
    "print(\"üß™ TESTING WITH SIMPLE MODEL FIRST...\")\n",
    "simple_model = create_simple_test_model(input_shape, num_classes)\n",
    "simple_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Simple model parameters: {simple_model.count_params():,}\")\n",
    "\n",
    "# Train for just 3 epochs to see if it learns at all\n",
    "print(\"Training simple model for 3 epochs to test data pipeline...\")\n",
    "simple_history = simple_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=min(50, len(train_ds)),  # Just 50 steps\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=min(10, len(val_ds)),  # Just 10 validation steps\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nSimple model results:\")\n",
    "print(f\"Epoch 1 accuracy: {simple_history.history['accuracy'][0]:.4f}\")\n",
    "print(f\"Epoch 3 accuracy: {simple_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Validation accuracy: {simple_history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "if simple_history.history['accuracy'][-1] > 0.25:\n",
    "    print(\"‚úÖ Simple model is learning - data pipeline looks good!\")\n",
    "    print(\"Issue is likely with the complex VGG16 model or learning rate\")\n",
    "else:\n",
    "    print(\"‚ùå Simple model also not learning - likely data issue!\")\n",
    "    print(\"Need to check data preprocessing and augmentation\")\n",
    "\n",
    "train_ds.reset()\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728aa52",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics with enhanced visualizations.\n",
    "    \"\"\"\n",
    "    # Create a larger figure with more subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    ax1.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "    ax1.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "    ax1.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add best accuracy annotation\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_val_acc_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "    ax1.annotate(f'Best: {best_val_acc:.3f}', \n",
    "                xy=(best_val_acc_epoch, best_val_acc), \n",
    "                xytext=(best_val_acc_epoch + 2, best_val_acc - 0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(epochs, history.history['loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    ax2.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        ax3.plot(epochs, history.history['lr'], 'g-', label='Learning Rate', linewidth=2)\n",
    "        ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Plot accuracy difference (overfitting indicator)\n",
    "        acc_diff = np.array(history.history['accuracy']) - np.array(history.history['val_accuracy'])\n",
    "        ax3.plot(epochs, acc_diff, 'purple', label='Training - Validation Accuracy', linewidth=2)\n",
    "        ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax3.set_title('Overfitting Indicator', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Accuracy Difference')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot smoothed validation accuracy trend\n",
    "    if len(history.history['val_accuracy']) > 5:\n",
    "        # Simple moving average\n",
    "        window_size = min(5, len(history.history['val_accuracy']) // 3)\n",
    "        val_acc_smooth = np.convolve(history.history['val_accuracy'], \n",
    "                                   np.ones(window_size)/window_size, mode='valid')\n",
    "        smooth_epochs = range(window_size, len(history.history['val_accuracy']) + 1)\n",
    "        \n",
    "        ax4.plot(epochs, history.history['val_accuracy'], 'lightcoral', alpha=0.5, label='Raw Validation Accuracy')\n",
    "        ax4.plot(smooth_epochs, val_acc_smooth, 'darkred', linewidth=3, label=f'Smoothed (window={window_size})')\n",
    "        ax4.set_title('Validation Accuracy Trend', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Validation Accuracy')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.set_ylim([0, 1])\n",
    "    else:\n",
    "        # If not enough epochs, show final metrics summary\n",
    "        ax4.text(0.1, 0.8, 'Training Summary', fontsize=16, fontweight='bold', transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.6, f'Epochs: {len(epochs)}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.5, f'Best Val Acc: {best_val_acc:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.4, f'Final Train Acc: {history.history[\"accuracy\"][-1]:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.3, f'Final Val Acc: {history.history[\"val_accuracy\"][-1]:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING HISTORY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total epochs trained: {len(epochs)}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_val_acc_epoch}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Calculate overfitting metrics\n",
    "    final_gap = history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Final accuracy gap: {final_gap:.4f}\")\n",
    "    if final_gap > 0.1:\n",
    "        print(\"‚ö†Ô∏è  Potential overfitting detected (gap > 0.1)\")\n",
    "    elif final_gap > 0.05:\n",
    "        print(\"‚ö° Mild overfitting (gap > 0.05)\")\n",
    "    else:\n",
    "        print(\"‚úÖ Good generalization (gap ‚â§ 0.05)\")\n",
    "    \n",
    "    # Training stability\n",
    "    last_5_val_acc = history.history['val_accuracy'][-5:] if len(history.history['val_accuracy']) >= 5 else history.history['val_accuracy']\n",
    "    val_acc_std = np.std(last_5_val_acc)\n",
    "    print(f\"Validation accuracy stability (last 5 epochs std): {val_acc_std:.4f}\")\n",
    "    \n",
    "    if val_acc_std < 0.01:\n",
    "        print(\"‚úÖ Training converged well\")\n",
    "    elif val_acc_std < 0.02:\n",
    "        print(\"‚ö° Training mostly stable\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Training still fluctuating\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e23fb",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3445bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set (since we don't have a separate test set)\n",
    "val_ds.reset()\n",
    "val_loss, val_accuracy = custom_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Generate predictions on validation set\n",
    "print(\"Generating predictions...\")\n",
    "val_ds.reset()\n",
    "y_pred = custom_model.predict(val_ds, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels from validation generator\n",
    "val_ds.reset()\n",
    "y_true_classes = []\n",
    "for i in range(len(val_ds)):\n",
    "    batch_images, batch_labels = next(val_ds)\n",
    "    batch_true_classes = np.argmax(batch_labels, axis=1)\n",
    "    y_true_classes.extend(batch_true_classes)\n",
    "\n",
    "y_true_classes = np.array(y_true_classes)\n",
    "\n",
    "# Ensure we have the same number of predictions and true labels\n",
    "min_length = min(len(y_pred_classes), len(y_true_classes))\n",
    "y_pred_classes = y_pred_classes[:min_length]\n",
    "y_true_classes = y_true_classes[:min_length]\n",
    "\n",
    "print(f\"Number of samples evaluated: {min_length}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Reset generator\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ae274",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d38999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Custom CNN')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {class_accuracy[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43257cc",
   "metadata": {},
   "source": [
    "## 9. Model Saving and Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14157c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_filename = f'../models/custom_vgg16_animals10_final.h5'\n",
    "custom_model.save(model_filename)\n",
    "print(f\"Model saved as: {model_filename}\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open(f'../models/custom_vgg16_animals10_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_name': 'Custom VGG16',\n",
    "    'dataset': 'Animals10',\n",
    "    'input_shape': input_shape,\n",
    "    'num_classes': num_classes,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs_trained': len(history.history['accuracy']),\n",
    "    'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "    'final_val_accuracy': val_accuracy,\n",
    "    'total_parameters': custom_model.count_params(),\n",
    "    'architecture': 'VGG16-inspired with Global Average Pooling'\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'../models/custom_vgg16_animals10_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CUSTOM VGG16 RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: Animals10\")\n",
    "print(f\"Architecture: Custom VGG16 with {custom_model.count_params():,} parameters\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Training epochs: {len(history.history['accuracy'])}\")\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Final validation accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Model saved: {model_filename}\")\n",
    "print(f\"Architecture follows VGG16 pattern:\")\n",
    "print(f\"  - 5 convolutional blocks (64‚Üí128‚Üí256‚Üí512‚Üí512 filters)\")\n",
    "print(f\"  - Global Average Pooling instead of FC layers\")\n",
    "print(f\"  - Dense(512) + Dense({num_classes}) classifier\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
