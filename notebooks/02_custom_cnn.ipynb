{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9023292",
   "metadata": {},
   "source": [
    "# Custom CNN Architecture for Image Classification\n",
    "\n",
    "This notebook implements and trains a custom CNN architecture from scratch.\n",
    "\n",
    "## Objectives:\n",
    "- Design custom CNN architecture\n",
    "- Implement data preprocessing and augmentation\n",
    "- Train the model with proper validation\n",
    "- Evaluate performance and visualize results\n",
    "- Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a473d",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (adjust based on your choice from notebook 01)\n",
    "DATASET_CHOICE = \"cifar10\"  # or \"animals10\"\n",
    "\n",
    "if DATASET_CHOICE == \"cifar10\":\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    num_classes = 10\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db73c0c",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d220e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Normalized data range: [{x_train.min():.3f}, {x_train.max():.3f}]\")\n",
    "print(f\"Labels shape: {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    x_train, y_train_cat, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set: {x_train_split.shape}\")\n",
    "print(f\"Validation set: {x_val_split.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the data generator\n",
    "datagen.fit(x_train_split)\n",
    "\n",
    "print(\"Data augmentation configured:\")\n",
    "print(f\"- Rotation: ±15 degrees\")\n",
    "print(f\"- Width/Height shift: ±10%\")\n",
    "print(f\"- Horizontal flip: True\")\n",
    "print(f\"- Zoom: ±10%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e2bd",
   "metadata": {},
   "source": [
    "## 3. Custom CNN Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a custom CNN architecture.\n",
    "    \n",
    "    Architecture pattern: Conv2D → BatchNorm → ReLU → MaxPool → Dropout\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(32, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Classification head\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = x_train.shape[1:]  # (32, 32, 3) for CIFAR-10\n",
    "custom_model = create_custom_cnn(input_shape, num_classes)\n",
    "\n",
    "# Display model architecture\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f069f0",
   "metadata": {},
   "source": [
    "## 4. Model Compilation and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "custom_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/custom_cnn_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Model compiled with:\")\n",
    "print(f\"- Optimizer: Adam\")\n",
    "print(f\"- Loss: Categorical Crossentropy\")\n",
    "print(f\"- Metrics: Accuracy\")\n",
    "print(f\"- Callbacks: Early Stopping, Model Checkpoint, LR Reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d267e",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Steps per epoch: {len(x_train_split) // BATCH_SIZE}\")\n",
    "\n",
    "# Train the model\n",
    "history = custom_model.fit(\n",
    "    datagen.flow(x_train_split, y_train_split, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(x_train_split) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_val_split, y_val_split),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728aa52",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax1.set_title('Model Accuracy Over Time')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax2.set_title('Model Loss Over Time')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best metrics\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_val_acc_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f} at epoch {best_val_acc_epoch}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e23fb",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3445bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = custom_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = custom_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ae274",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d38999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Custom CNN')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {class_accuracy[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43257cc",
   "metadata": {},
   "source": [
    "## 9. Model Saving and Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14157c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model_filename = f'../models/custom_cnn_{DATASET_CHOICE}_final.h5'\n",
    "custom_model.save(model_filename)\n",
    "print(f\"Model saved as: {model_filename}\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open(f'../models/custom_cnn_{DATASET_CHOICE}_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# Results summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CUSTOM CNN RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: {DATASET_CHOICE.upper()}\")\n",
    "print(f\"Architecture: Custom CNN with {custom_model.count_params():,} parameters\")\n",
    "print(f\"Training epochs: {len(history.history['accuracy'])}\")\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Model saved: {model_filename}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
