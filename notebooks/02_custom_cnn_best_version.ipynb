{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9023292",
   "metadata": {},
   "source": [
    "# Custom CNN Architecture for Image Classification\n",
    "\n",
    "This notebook implements and trains a custom CNN architecture from scratch.\n",
    "\n",
    "## Objectives:\n",
    "- Design custom CNN architecture\n",
    "- Implement data preprocessing and augmentation\n",
    "- Train the model with proper validation\n",
    "- Evaluate performance and visualize results\n",
    "- Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a473d",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cea7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU configured: 1 device(s) found\n",
      "TensorFlow version: 2.15.0\n",
      "Available devices: ['/physical_device:CPU:0', '/physical_device:GPU:0']\n",
      "✅ Random seeds set for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Configure environment for Apple Silicon optimization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
    "# Configure TensorFlow for Apple Silicon\n",
    "try:\n",
    "    # Enable memory growth for GPU (if available)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ GPU configured: {len(gpus)} device(s) found\")\n",
    "    else:\n",
    "        print(\"ℹ️  No GPU found, using CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  GPU configuration warning: {e}\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Available devices: {[device.name for device in tf.config.list_physical_devices()]}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "print(\"✅ Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db73c0c",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bb04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/smithn5/.cache/kagglehub/datasets/alessiocorrado99/animals10/versions/2'\n",
    "data_dir = os.path.join(path, 'raw-img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e3726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training, validation, and test sets physically in the data directory\n",
    "# previous_weights_path = '../models/custom_costum_animals10_best.h5'\n",
    "base_dir = '../data/'\n",
    "train_ratio = 0.9\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.0\n",
    "batch_size = 32\n",
    "\n",
    "img_height = 128 # for basic cnn\n",
    "img_width = 128 # for basic cnn\n",
    "\n",
    "# Create base directories for splits\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974f01c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m test_images = images[n_train+n_val:]\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m train_images:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m val_images:\n\u001b[32m     22\u001b[39m     shutil.copy(os.path.join(class_path, img), os.path.join(base_dir, \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m, class_name, img))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/shutil.py:431\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    430\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m copymode(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/shutil.py:256\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    254\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# now split and copy images\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    \n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    n_total = len(images)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_test = n_total - n_train - n_val  # Ensure all images are used\n",
    "\n",
    "    train_images = images[:n_train]\n",
    "    val_images = images[n_train:n_train+n_val]\n",
    "    test_images = images[n_train+n_val:]\n",
    "\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(base_dir, 'train', class_name, img))\n",
    "    for img in val_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(base_dir, 'val', class_name, img))\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(base_dir, 'test', class_name, img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d220e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating improved data generators...\n",
      "Found 23556 images belonging to 10 classes.\n",
      "Found 2614 images belonging to 10 classes.\n",
      "Found 9 images belonging to 10 classes.\n",
      "Found 10 classes: ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
      "Training samples: 23556\n",
      "Validation samples: 2614\n",
      "Test samples: 9\n",
      "Training batches per epoch: 737\n",
      "Validation batches per epoch: 82\n",
      "Test batches per epoch: 1\n",
      "Batch size: 32\n",
      "\n",
      "Class distribution check:\n",
      "  cane: 4863 images\n",
      "  cavallo: 2623 images\n",
      "  elefante: 1446 images\n",
      "  farfalla: 2112 images\n",
      "  gallina: 3098 images\n",
      "  gatto: 1668 images\n",
      "  mucca: 1866 images\n",
      "  pecora: 1820 images\n",
      "  ragno: 4821 images\n",
      "  scoiattolo: 1862 images\n",
      "\n",
      "Class balance analysis:\n",
      "  Min class size: 1446\n",
      "  Max class size: 4863\n",
      "  Imbalance ratio: 3.36\n",
      "⚡ Moderate class imbalance detected\n",
      "\n",
      "✅ Improved data generators created with:\n",
      "  - Reduced augmentation intensity\n",
      "  - Separate validation generator (no augmentation)\n",
      "  - Proper class balance verification\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using tf.keras.preprocessing.image.ImageDataGenerator\n",
    "\n",
    "base_dir = '../data/'\n",
    "\n",
    "\n",
    "# ISSUE DIAGNOSIS: Your augmentation might be too aggressive!\n",
    "# Let's create a less aggressive augmentation setup\n",
    "\n",
    "print(\"🔧 Creating improved data generators...\")\n",
    "\n",
    "# Create LESS aggressive augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize to [0,1]\n",
    "    rotation_range=20,      # Reduced from 20\n",
    "    width_shift_range=0.1,  # Reduced from 0.2\n",
    "    height_shift_range=0.1, # Reduced from 0.2\n",
    "    shear_range=0.1,     # Reduced from 0.2\n",
    "    zoom_range=0.1,         # Reduced from 0.2\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create validation generator without augmentation (IMPORTANT!)\n",
    "test_val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Only rescaling for validation\n",
    "    rotation_range=20,      # Reduced from 20\n",
    "    width_shift_range=0.1,  # Reduced from 0.2\n",
    "    height_shift_range=0.1, # Reduced from 0.2\n",
    "    shear_range=0.1,     # Reduced from 0.2\n",
    "    zoom_range=0.1,         # Reduced from 0.2\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "    \n",
    ")\n",
    "\n",
    "# Create training dataset with augmentation\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    os.path.join(base_dir, 'train'),\n",
    "    shuffle=True,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create validation dataset WITHOUT augmentation\n",
    "val_ds = test_val_datagen.flow_from_directory(\n",
    "    os.path.join(base_dir, 'val'),\n",
    "    shuffle=False,  # Don't shuffle validation\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "#create test dataset WITHOUT augmentation\n",
    "test_ds = test_val_datagen.flow_from_directory(\n",
    "    os.path.join(base_dir, 'test'),\n",
    "    shuffle=False,  # Don't shuffle test\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = list(train_ds.class_indices.keys())\n",
    "print(f\"Found {len(class_names)} classes: {class_names}\")\n",
    "\n",
    "# Calculate dataset sizes\n",
    "print(f\"Training samples: {train_ds.samples}\")\n",
    "print(f\"Validation samples: {val_ds.samples}\")\n",
    "print(f\"Test samples: {test_ds.samples}\")\n",
    "print(f\"Training batches per epoch: {len(train_ds)}\")\n",
    "print(f\"Validation batches per epoch: {len(val_ds)}\")\n",
    "print(f\"Test batches per epoch: {len(test_ds)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Verify class balance\n",
    "print(f\"\\nClass distribution check:\")\n",
    "class_counts = {}\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(path, 'raw-img', class_name)\n",
    "    if os.path.exists(class_dir):\n",
    "        count = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        class_counts[class_name] = count\n",
    "        print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "# Check for extremely imbalanced classes\n",
    "min_count = min(class_counts.values())\n",
    "max_count = max(class_counts.values())\n",
    "imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "print(f\"\\nClass balance analysis:\")\n",
    "print(f\"  Min class size: {min_count}\")\n",
    "print(f\"  Max class size: {max_count}\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"⚠️  WARNING: Severe class imbalance detected!\")\n",
    "    print(\"   This could explain poor learning performance\")\n",
    "elif imbalance_ratio > 3:\n",
    "    print(\"⚡ Moderate class imbalance detected\")\n",
    "else:\n",
    "    print(\"✅ Classes are reasonably balanced\")\n",
    "\n",
    "print(f\"\\n✅ Improved data generators created with:\")\n",
    "print(f\"  - Reduced augmentation intensity\")\n",
    "print(f\"  - Separate validation generator (no augmentation)\")\n",
    "print(f\"  - Proper class balance verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e2bd",
   "metadata": {},
   "source": [
    "## 3. Custom CNN Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 224, 224, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 256)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              263168    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1700394 (6.49 MB)\n",
      "Trainable params: 1700394 (6.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Model Details:\n",
      "Input shape: (224, 224, 3)\n",
      "Number of classes: 10\n",
      "Total parameters: 1,700,394\n",
      "Trainable parameters: 1,700,394\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "✅ Model architecture diagram saved to '../models/simple_cnn_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "def create_custom_made_cnn(input_shape, num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a simple CNN model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "    # block 1: 32 filtros\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    # block 2: 64 filtros\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # block 3: 128 filtros\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # block 4: 256 filtros\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Capas densas\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "     # the one before was\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (img_height, img_width, 3)\n",
    "num_classes = len(class_names)\n",
    "custom_model = create_custom_made_cnn(input_shape, num_classes)\n",
    "\n",
    "# Display model architecture\n",
    "custom_model.summary()\n",
    "\n",
    "print(f\"\\nModel Details:\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Total parameters: {custom_model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in custom_model.trainable_weights]):,}\")\n",
    "\n",
    "# Visualize model architecture\n",
    "tf.keras.utils.plot_model(\n",
    "    custom_model,\n",
    "    to_file='../models/simple_cnn_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")\n",
    "print(\"✅ Model architecture diagram saved to '../models/simple_cnn_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f069f0",
   "metadata": {},
   "source": [
    "## 4. Model Compilation and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea466a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUGGING MODEL AND DATA...\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🚀 STARTING IMPROVED TRAINING...\n",
      "==================================================\n",
      "✅ Model compiled with reduced learning rate: 0.0002\n",
      "✅ Training callbacks configured\n",
      "   - EarlyStopping: patience=10\n",
      "   - ModelCheckpoint: saves best model\n",
      "   - ReduceLROnPlateau: factor=0.1, patience=5\n"
     ]
    }
   ],
   "source": [
    "# Let's first debug the data and model setup\n",
    "print(\"🔍 DEBUGGING MODEL AND DATA...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 STARTING IMPROVED TRAINING...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# STEP 1: Compile the model FIRST\n",
    "custom_model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0002),  \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(\"✅ Model compiled with reduced learning rate: 0.0002\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# STEP 2: Setup callbacks (ModelCheckpoint will save best weights)\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,  # Reduced patience\n",
    "        restore_best_weights=True,\n",
    "        # verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/best_model_128.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,  # Save full model\n",
    "        # verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,  # More aggressive reduction\n",
    "        patience=2,  # slower response\n",
    "        min_lr=1e-6,\n",
    "        # verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✅ Training callbacks configured\")\n",
    "print(\"   - EarlyStopping: patience=10\")\n",
    "print(\"   - ModelCheckpoint: saves best model\")\n",
    "print(\"   - ReduceLROnPlateau: factor=0.1, patience=5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d267e",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved training setup:\n",
      "- Learning rate: 0.0001 (reduced from 0.001)\n",
      "- Max epochs: 35\n",
      "- Early stopping patience: 8\n",
      "- LR reduction factor: 0.2 (more aggressive)\n",
      "- Batch size: 32\n",
      "Epoch 1/35\n",
      "737/737 [==============================] - 524s 710ms/step - loss: 2.2184 - accuracy: 0.1879 - val_loss: 2.1533 - val_accuracy: 0.1955 - lr: 2.0000e-04\n",
      "Epoch 2/35\n",
      "737/737 [==============================] - 210s 284ms/step - loss: 2.3224 - accuracy: 0.1819 - val_loss: 2.3024 - val_accuracy: 0.2238 - lr: 2.0000e-04\n",
      "Epoch 3/35\n",
      "737/737 [==============================] - 212s 287ms/step - loss: 2.6791 - accuracy: 0.1720 - val_loss: 2.2856 - val_accuracy: 0.2039 - lr: 2.0000e-04\n",
      "Epoch 4/35\n",
      "137/737 [====>.........................] - ETA: 2:57 - loss: 2.9897 - accuracy: 0.1535"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- Batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train with improved settings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m history = \u001b[43mcustom_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#steps_per_epoch=len(train_ds),\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#validation_steps=len(val_ds),\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1813\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1811\u001b[39m logs = tmp_logs\n\u001b[32m   1812\u001b[39m end_step = step + data_handler.step_increment\n\u001b[32m-> \u001b[39m\u001b[32m1813\u001b[39m \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n\u001b[32m   1815\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/callbacks.py:475\u001b[39m, in \u001b[36mCallbackList.on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[32m    469\u001b[39m \n\u001b[32m    470\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[33;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_call_train_batch_hooks:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/callbacks.py:322\u001b[39m, in \u001b[36mCallbackList._call_batch_hook\u001b[39m\u001b[34m(self, mode, hook, batch, logs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m._call_batch_begin_hook(mode, batch, logs)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m hook == \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    325\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    326\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mExpected values are [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbegin\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    327\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/callbacks.py:345\u001b[39m, in \u001b[36mCallbackList._call_batch_end_hook\u001b[39m\u001b[34m(self, mode, batch, logs)\u001b[39m\n\u001b[32m    342\u001b[39m     batch_time = time.time() - \u001b[38;5;28mself\u001b[39m._batch_start_time\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m._batch_times.append(batch_time)\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._batch_times) >= \u001b[38;5;28mself\u001b[39m._num_batches_for_timing_check:\n\u001b[32m    348\u001b[39m     end_hook_name = hook_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/callbacks.py:393\u001b[39m, in \u001b[36mCallbackList._call_batch_hook_helper\u001b[39m\u001b[34m(self, hook_name, batch, logs)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m    392\u001b[39m     hook = \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_timing:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hook_times:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/callbacks.py:1093\u001b[39m, in \u001b[36mProgbarLogger.on_train_batch_end\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/callbacks.py:1169\u001b[39m, in \u001b[36mProgbarLogger._batch_update_progbar\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28mself\u001b[39m.seen += add_seen\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose == \u001b[32m1\u001b[39m:\n\u001b[32m   1168\u001b[39m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     logs = \u001b[43mtf_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28mself\u001b[39m.progbar.update(\u001b[38;5;28mself\u001b[39m.seen, \u001b[38;5;28mlist\u001b[39m(logs.items()), finalize=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/utils/tf_utils.py:694\u001b[39m, in \u001b[36msync_to_numpy_or_python_type\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t.item() \u001b[38;5;28;01mif\u001b[39;00m np.ndim(t) == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/tensorflow/python/util/nest.py:631\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structure, **kwargs)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnest.map_structure\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap_structure\u001b[39m(func, *structure, **kwargs):\n\u001b[32m    547\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[32m    548\u001b[39m \n\u001b[32m    549\u001b[39m \u001b[33;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m \u001b[33;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModality\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1066\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(modality, func, *structure, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[32m    970\u001b[39m \n\u001b[32m    971\u001b[39m \u001b[33;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1063\u001b[39m \u001b[33;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modality == Modality.CORE:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m modality == Modality.DATA:\n\u001b[32m   1068\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, *structure, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[39m, in \u001b[36m_tf_core_map_structure\u001b[39m\u001b[34m(func, *structure, **kwargs)\u001b[39m\n\u001b[32m   1101\u001b[39m flat_structure = (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[32m   1102\u001b[39m entries = \u001b[38;5;28mzip\u001b[39m(*flat_structure)\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[32m   1105\u001b[39m     structure[\u001b[32m0\u001b[39m],\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m     \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m   1107\u001b[39m     expand_composites=expand_composites,\n\u001b[32m   1108\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1101\u001b[39m flat_structure = (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[32m   1102\u001b[39m entries = \u001b[38;5;28mzip\u001b[39m(*flat_structure)\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[32m   1105\u001b[39m     structure[\u001b[32m0\u001b[39m],\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[32m   1107\u001b[39m     expand_composites=expand_composites,\n\u001b[32m   1108\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/keras/src/utils/tf_utils.py:687\u001b[39m, in \u001b[36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[32m    685\u001b[39m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         t = \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[32m    689\u001b[39m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np.ndarray, np.generic)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:394\u001b[39m, in \u001b[36m_EagerTensorBase.numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[32m    372\u001b[39m \n\u001b[32m    373\u001b[39m \u001b[33;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m \u001b[33;03m    NumPy dtype.\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m maybe_arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np.ndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ironhack/image_class_project_ih/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:360\u001b[39m, in \u001b[36m_EagerTensorBase._numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    359\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters\n",
    "EPOCHS = 35  # Increased epochs since we lowered LR\n",
    "\n",
    "print(f\"Improved training setup:\")\n",
    "print(f\"- Learning rate: 0.0001 (reduced from 0.001)\")\n",
    "print(f\"- Max epochs: {EPOCHS}\")\n",
    "print(f\"- Early stopping patience: 8\")\n",
    "print(f\"- LR reduction factor: 0.2 (more aggressive)\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "\n",
    "# Train with improved settings\n",
    "history = custom_model.fit(\n",
    "    train_ds,\n",
    "    #steps_per_epoch=len(train_ds),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    #validation_steps=len(val_ds),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training completed!\")\n",
    "if len(history.history['accuracy']) > 0:\n",
    "    print(f\"Total epochs trained: {len(history.history['accuracy'])}\")\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# Reset generators for future use\n",
    "train_ds.reset()\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728aa52",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565976c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics with enhanced visualizations.\n",
    "    \"\"\"\n",
    "    # Create a larger figure with more subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    ax1.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "    ax1.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "    ax1.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add best accuracy annotation\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_val_acc_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "    ax1.annotate(f'Best: {best_val_acc:.3f}', \n",
    "                xy=(best_val_acc_epoch, best_val_acc), \n",
    "                xytext=(best_val_acc_epoch + 2, best_val_acc - 0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(epochs, history.history['loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    ax2.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        ax3.plot(epochs, history.history['lr'], 'g-', label='Learning Rate', linewidth=2)\n",
    "        ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Plot accuracy difference (overfitting indicator)\n",
    "        acc_diff = np.array(history.history['accuracy']) - np.array(history.history['val_accuracy'])\n",
    "        ax3.plot(epochs, acc_diff, 'purple', label='Training - Validation Accuracy', linewidth=2)\n",
    "        ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax3.set_title('Overfitting Indicator', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Accuracy Difference')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot smoothed validation accuracy trend\n",
    "    if len(history.history['val_accuracy']) > 5:\n",
    "        # Simple moving average\n",
    "        window_size = min(5, len(history.history['val_accuracy']) // 3)\n",
    "        val_acc_smooth = np.convolve(history.history['val_accuracy'], \n",
    "                                   np.ones(window_size)/window_size, mode='valid')\n",
    "        smooth_epochs = range(window_size, len(history.history['val_accuracy']) + 1)\n",
    "        \n",
    "        ax4.plot(epochs, history.history['val_accuracy'], 'lightcoral', alpha=0.5, label='Raw Validation Accuracy')\n",
    "        ax4.plot(smooth_epochs, val_acc_smooth, 'darkred', linewidth=3, label=f'Smoothed (window={window_size})')\n",
    "        ax4.set_title('Validation Accuracy Trend', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Validation Accuracy')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.set_ylim([0, 1])\n",
    "    else:\n",
    "        # If not enough epochs, show final metrics summary\n",
    "        ax4.text(0.1, 0.8, 'Training Summary', fontsize=16, fontweight='bold', transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.6, f'Epochs: {len(epochs)}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.5, f'Best Val Acc: {best_val_acc:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.4, f'Final Train Acc: {history.history[\"accuracy\"][-1]:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.text(0.1, 0.3, f'Final Val Acc: {history.history[\"val_accuracy\"][-1]:.4f}', fontsize=12, transform=ax4.transAxes)\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING HISTORY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total epochs trained: {len(epochs)}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f} at epoch {best_val_acc_epoch}\")\n",
    "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Calculate overfitting metrics\n",
    "    final_gap = history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Final accuracy gap: {final_gap:.4f}\")\n",
    "    if final_gap > 0.1:\n",
    "        print(\"⚠️  Potential overfitting detected (gap > 0.1)\")\n",
    "    elif final_gap > 0.05:\n",
    "        print(\"⚡ Mild overfitting (gap > 0.05)\")\n",
    "    else:\n",
    "        print(\"✅ Good generalization (gap ≤ 0.05)\")\n",
    "    \n",
    "    # Training stability\n",
    "    last_5_val_acc = history.history['val_accuracy'][-5:] if len(history.history['val_accuracy']) >= 5 else history.history['val_accuracy']\n",
    "    val_acc_std = np.std(last_5_val_acc)\n",
    "    print(f\"Validation accuracy stability (last 5 epochs std): {val_acc_std:.4f}\")\n",
    "    \n",
    "    if val_acc_std < 0.01:\n",
    "        print(\"✅ Training converged well\")\n",
    "    elif val_acc_std < 0.02:\n",
    "        print(\"⚡ Training mostly stable\")\n",
    "    else:\n",
    "        print(\"⚠️  Training still fluctuating\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e23fb",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3445bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set (since we don't have a separate test set)\n",
    "val_ds.reset()\n",
    "val_loss, val_accuracy = custom_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Generate predictions on validation set\n",
    "print(\"Generating predictions...\")\n",
    "val_ds.reset()\n",
    "y_pred = custom_model.predict(val_ds, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels from validation generator\n",
    "val_ds.reset()\n",
    "y_true_classes = []\n",
    "for i in range(len(val_ds)):\n",
    "    batch_images, batch_labels = next(val_ds)\n",
    "    batch_true_classes = np.argmax(batch_labels, axis=1)\n",
    "    y_true_classes.extend(batch_true_classes)\n",
    "\n",
    "y_true_classes = np.array(y_true_classes)\n",
    "\n",
    "# Ensure we have the same number of predictions and true labels\n",
    "min_length = min(len(y_pred_classes), len(y_true_classes))\n",
    "y_pred_classes = y_pred_classes[:min_length]\n",
    "y_true_classes = y_true_classes[:min_length]\n",
    "\n",
    "print(f\"Number of samples evaluated: {min_length}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Reset generator\n",
    "val_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ae274",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d38999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Custom CNN')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {class_accuracy[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43257cc",
   "metadata": {},
   "source": [
    "## 9. Model Saving and Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14157c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy saving and documentation\n",
    "val_accuracy_str = f\"{val_accuracy:.2f}\".replace('.', '_')\n",
    "# Save the final model\n",
    "model_filename = f'../models/custom_costum_animals10_acc_{val_accuracy_str}.h5'\n",
    "custom_model.save(model_filename)\n",
    "print(f\"Model saved as: {model_filename}\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open(f'../models/custom_costum_animals10_acc_{val_accuracy_str}_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"Training history saved\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_name': 'Custom CNN',\n",
    "    'dataset': 'Animals10',\n",
    "    'input_shape': input_shape,\n",
    "    'num_classes': num_classes,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs_trained': len(history.history['accuracy']),\n",
    "    'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "    'final_val_accuracy': val_accuracy,\n",
    "    'total_parameters': custom_model.count_params(),\n",
    "    'architecture': 'VGG16-inspired with Global Average Pooling'\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'../models/custom_costum_animals10_acc_{val_accuracy_str}_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CUSTOM CNN RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: Animals10\")\n",
    "print(f\"Architecture: Custom CNN with {custom_model.count_params():,} parameters\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Training epochs: {len(history.history['accuracy'])}\")\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Final validation accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Model saved: {model_filename}\")\n",
    "print(f\"Architecture follows CNN pattern:\")\n",
    "print(f\"  - 5 convolutional blocks (64→128→256→512→512 filters)\")\n",
    "print(f\"  - Global Average Pooling instead of FC layers\")\n",
    "print(f\"  - Dense(512) + Dense({num_classes}) classifier\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122994dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
